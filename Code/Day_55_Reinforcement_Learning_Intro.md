# Day 55: 강화 학습 소개 (Introduction to Reinforcement Learning)

## 학습 목표
- 강화 학습(RL)의 기본 개념 이해
- RL의 주요 용어(에이전트, 환경, 상태, 행동, 보상, 정책, 가치 함수, 모델) 숙지

## 핵심 개념

### 1. 강화 학습 (Reinforcement Learning)
- 에이전트(Agent)가 환경(Environment)과 상호작용하며 학습하는 머신러닝의 한 분야입니다.
- 에이전트는 현재 상태(State)를 관찰하고 행동(Action)을 선택합니다.
- 환경은 에이전트의 행동에 따라 다음 상태로 전이되고 보상(Reward)을 에이전트에게 제공합니다.
- 에이전트의 목표는 누적 보상을 최대화하는 정책(Policy)을 학습하는 것입니다.

### 2. 주요 구성 요소
    - **에이전트 (Agent)**: 학습의 주체. 환경을 관찰하고, 행동을 결정하며, 보상을 통해 학습합니다.
    - **환경 (Environment)**: 에이전트가 상호작용하는 외부 세계. 에이전트의 행동에 따라 상태가 변하고 보상을 제공합니다.
    - **상태 (State, S)**: 특정 시점에서 환경에 대한 관찰 가능한 정보. 에이전트가 행동을 결정하는 데 사용됩니다. (예: 게임 화면, 로봇 센서 값)
    - **행동 (Action, A)**: 에이전트가 특정 상태에서 취할 수 있는 움직임이나 결정. (예: 게임 캐릭터의 이동, 로봇 팔의 움직임)
    - **보상 (Reward, R)**: 에이전트가 특정 행동을 취했을 때 환경으로부터 받는 피드백. 즉각적인 좋고 나쁨을 나타내는 스칼라 값입니다. (예: 게임 점수 획득(+), 장애물 충돌(-))
    - **정책 (Policy, π)**: 특정 상태에서 에이전트가 어떤 행동을 선택할지에 대한 전략 또는 규칙. 상태를 행동으로 매핑하는 함수로 표현될 수 있습니다. (π(a|s) = P[A_t = a | S_t = s])
    - **가치 함수 (Value Function, V, Q)**: 특정 상태 또는 특정 상태-행동 쌍의 장기적인 가치를 평가하는 함수. 미래에 받을 누적 보상의 기댓값으로 정의됩니다.
        - 상태 가치 함수 V(s): 상태 s에서 시작하여 특정 정책 π를 따랐을 때 받을 수 있는 누적 보상의 기댓값.
        - 행동 가치 함수 Q(s, a): 상태 s에서 행동 a를 취하고 이후 특정 정책 π를 따랐을 때 받을 수 있는 누적 보상의 기댓값.
    - **모델 (Model)** (선택 사항): 환경이 어떻게 작동하는지에 대한 에이전트의 표현. 상태 전이 확률과 보상 함수를 포함할 수 있습니다.
        - 모델 기반 RL: 환경의 모델을 학습하거나 알고 있는 경우.
        - 모델 프리 RL: 환경의 모델 없이 학습하는 경우.

### 3. 강화 학습의 목표
- 에이전트가 장기적으로 받는 누적 보상(Cumulative Reward)을 최대화하는 최적의 정책(Optimal Policy, π*)을 찾는 것입니다.

## 추가 학습 자료
- [위키백과: 강화 학습](https://ko.wikipedia.org/wiki/%EA%B0%95%ED%99%94_%ED%95%99%EC%8A%B5)
- [Sutton and Barto, Reinforcement Learning: An Introduction (번역본 있음)](http://incompleteideas.net/book/the-book-2nd.html)

## 다음 학습 내용
- Day 56: 마르코프 결정 과정 (Markov Decision Processes)
